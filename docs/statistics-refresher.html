<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Statistics Refresher | Intuitive Metrics</title>
  <meta name="description" content="Chapter 1 Statistics Refresher | Intuitive Metrics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Statistics Refresher | Intuitive Metrics" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Statistics Refresher | Intuitive Metrics" />
  
  
  

<meta name="author" content="Benjamin Elsner" />


<meta name="date" content="2022-09-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NCGQKDQN8C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NCGQKDQN8C');
</script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intuitive Metrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="statistics-refresher.html"><a href="statistics-refresher.html"><i class="fa fa-check"></i><b>1</b> Statistics Refresher</a>
<ul>
<li class="chapter" data-level="1.1" data-path="statistics-refresher.html"><a href="statistics-refresher.html#lingo-estimators-estimates-and-all-that"><i class="fa fa-check"></i><b>1.1</b> Lingo: Estimators, Estimates and all that</a></li>
<li class="chapter" data-level="1.2" data-path="statistics-refresher.html"><a href="statistics-refresher.html#the-sampling-distribution"><i class="fa fa-check"></i><b>1.2</b> The Sampling Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="statistics-refresher.html"><a href="statistics-refresher.html#the-law-of-large-numbers-and-consistency-of-an-estimator"><i class="fa fa-check"></i><b>1.3</b> The Law of Large Numbers and Consistency of an Estimator</a></li>
<li class="chapter" data-level="1.4" data-path="statistics-refresher.html"><a href="statistics-refresher.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>1.4</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intuitive Metrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics-refresher" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Statistics Refresher<a href="statistics-refresher.html#statistics-refresher" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter covers statistical concepts that are central in any statistical analysis. We will use the example of dice rolls to develop an intuition for these concepts.</p>
<div id="lingo-estimators-estimates-and-all-that" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Lingo: Estimators, Estimates and all that<a href="statistics-refresher.html#lingo-estimators-estimates-and-all-that" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In most (if not all) statistical analyses, we want to learn something about a <strong>population of interest</strong>. The population of interest does not need to be the actual population of a country or the world, but it can also be a very specific population such as all students in this course, all people below age 25, all countries (or only those with a GDP less than 20k€), etc. However, using the entire population for an analysis is usually not practical and often impossible. For example, if you were to do a blood test with the entire population – all the blood in a patient’s body – you would get the best possible information about the population of interest, but you can also be sure that the patient won’t survive the test. For this reason, blood tests are based on a small sample of blood. As we will learn, much can be learned from relatively small samples about the population.</p>
<p>Within the population of interest, we are typically interested in a particular statistic – which is sometimes called <strong>target parameter</strong> or <strong>quantity of interest</strong> or <strong>population statistic</strong> or <strong>estimand</strong>. Your doctor wants to know the concentration of certain biomarkers in all your blood cells. As a policy analyst, we want to know the impact of childcare subsidies on child development, etc. These are examples of population statistics we are interested in.</p>
<p>If our analysis is based on a sample rather than the population, we say that we <strong>estimate</strong> these population statistics. This means that we compute the same statistic in our sample. The procedure we use to do this is called <strong>estimator</strong> and the result, i.e. the statistic computed from the sample, is called <strong>estimate</strong>. Once we obtained an estimate, we use insights from statistics to assess how much we can learn from this estimate about the population statistic we are interested in.</p>
<p>For example, we may be interested in the population mean <span class="math inline">\(\mu_Y\)</span>. We can draw a random sample of size <span class="math inline">\(n\)</span> from the population and estimate the population mean using the formula for the sample mean,</p>
<p><span class="math display" id="eq:mean">\[\begin{equation}
\overline{Y}=\frac{1}{n}\displaystyle\sum_{i=1}^n Y_i.
\tag{1.1}
\end{equation}\]</span></p>
<p>The formula in Equation ((eq:mean)) is the <strong>estimator</strong> for the population mean <span class="math inline">\(\mu_Y\)</span>. If we apply the estimator to a <strong>particular</strong> sample <span class="math inline">\(s\)</span>, we obtain an <strong>estimate</strong> <span class="math inline">\(\overline{Y_s}\)</span>for the sample <span class="math inline">\(s\)</span>. If we were to draw another sample <span class="math inline">\(t \neq s\)</span>, we use the same estimator but obtain a different estimate <span class="math inline">\(\overline{Y_t}\)</span>.</p>
<div class="blackbox">
<div class="center">
<p><strong>Definition: Estimator and Estimate</strong></p>
</div>
<p>An <strong>estimator</strong> is a procedure that is used for calculating an estimate for a quantity of of interest based on a sample from the population of interest.</p>
<p><strong>In plain English</strong>: An estimator is to an estimate what a recipe is to a cake. Suppose we want to bake a chocolate cake (that’s the quantity, or in that case rather the object of interest). We then use recipe that tells us how we convert the ingredients into a chocolate cake. The recipe is the equivalent of the estimator. When we do this, we hopefully get a tasty chocolate cake, which is the estimate. But every time we do this, we get a cake that tastes slightly different. It’s the same with estimators. Not that we know what they taste like, but we get a different estimate every time we apply the same estimator to a different sample.</p>
</div>
</div>
<div id="the-sampling-distribution" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> The Sampling Distribution<a href="statistics-refresher.html#the-sampling-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The sampling distribution is central to any statistical analysis. After all, we typically cannot analyse the entire population of interest but have to draw inference from a smaller random sample. As we will see, there is a lot that can be learned about the population even from a medium-sized random sample. However, an analysis that is based on random sampling comes with the challenge that every random sample will yield different estimates. Suppose we wanted to estimate the mean height of students at UCD. If we draw five students at random and calculate the mean height of these students, we may end up with an estimate that is larger than the true mean – if we happen to disproportionately sample taller students – or smaller than the true mean – if we happen to disproportionately sample shorter students. If we draw many such samples and compute the mean in each sample, we will likely get a different mean height in each sample. <strong>The distribution of these means is called the sampling distribution.</strong></p>
<div class="blackbox">
<div class="center">
<p><strong>Definition: Sampling Distribution</strong></p>
</div>
<p><strong>Wikipedia says</strong>: <em>A sampling distribution or finite-sample distribution is the probability distribution of a given random-sample-based statistic.</em></p>
<p><strong>In plain English</strong>: Suppose we draw many random samples and in each sample we compute the same statistic (for example the mean, variance, minimum or maximum). The distribution of these statistics across all samples is called the sampling distribution.</p>
</div>
<p>When students encounter the sampling distribution for the first time, they tend to struggle with this concept. To illustrate what the sampling distribution is, where it comes from and what it is useful for, we will consider <strong>dice rolls</strong>. If we have a fair dice, the <strong>expected number of points is 3.5</strong>. This is the case because each side comes up with a probability of <span class="math inline">\(1/6\)</span>, and thus <strong>the (population) mean is</strong></p>
<p><span class="math display">\[\begin{equation*}
\mu_Y=\frac{1}{6}\times \left(1 + 2 + 3 + 4 + 5 + 6\right)=3.5.
\end{equation*}\]</span></p>
<p>This mean is what we will get if we roll a dice an infinite number of times and calculate the mean across all rolls. But that’s a theoretical quantity. No one can actually roll a dice so many times. The real question here is what happens if we roll a dice a finite number of times and take the mean. In that case we need to compute the sample mean <span class="math inline">\(\overline{Y}\)</span> over all <span class="math inline">\(i \in \{1, \dots, n\}\)</span> dice rolls</p>
<p><span class="math display">\[\begin{equation*}
\overline{Y}=\frac{1}{n}\displaystyle\sum_{i=1}^n Y_i,
\end{equation*}\]</span></p>
<p>
whereby <span class="math inline">\(Y_i\)</span> are the points from an individual diceroll.</p>
<p><span class="math display">\[\\[0.3cm]\]</span></p>
<p>We can also compute the <strong>population variance of a dice roll</strong> for <span class="math inline">\(Y \in \{1,2,3,4,5,6\}\)</span> (remember: population here means an infinite number of dice rolls):</p>
<p><span class="math display">\[\begin{equation*}
\sigma_Y^2=\frac{1}{6} \displaystyle\sum_{i=1}^6 (Y_i-\mu_Y)^2=\approx 2.92.
\end{equation*}\]</span></p>
<p>Need a <em>refresher of expected value and variance? Check out this video</em>: <a href="https://www.youtube.com/watch?v=OvTEhNL96v0">https://www.youtube.com/watch?v=OvTEhNL96v0</a>.</p>
<div id="sampling-distribution-in-very-small-samples" class="section level3 unnumbered hasAnchor">
<h3>Sampling distribution in very small samples<a href="statistics-refresher.html#sampling-distribution-in-very-small-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s start with a small sample size: we roll two dice, compute the mean, roll two dice, compute the mean, and so on. Figure <a href="statistics-refresher.html#fig:diceroll2">1.1</a> shows the result of this exercise with 200 samples (i.e. 200 times we roll two dice and compute the mean). The orange panels at the top show the points of the two dice. With every sample of size two, we add a new mean to the distribution shown in the panel at the bottom (the grey line highlights where in the distribution the new mean falls).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diceroll2"></span>
<img src="docs/_bookdown_files/diceroll2.gif" alt="Sampling Distribution: Two Dice Rolls per Sample" width="40%" />
<p class="caption">
Figure 1.1: Sampling Distribution: Two Dice Rolls per Sample
</p>
</div>
<p>The distribution that emerges – the sampling distribution – is quite interesting. The distribution seems to be fairly symmetric around the population mean of 3.5, and there are more samples with means between 2 and 4 and fewer with means smaller than 2 or larger than 4. This distribution should not come as a surprise because there are more combinations that yield a sample mean of 3.5 – <span class="math inline">\(\{3,4\}\)</span>, <span class="math inline">\(\{2,5\}\)</span>, <span class="math inline">\(\{1,6\}\)</span> all yield a sample mean of 3.5, whereas a sample mean of 1 can only occur with the combination <span class="math inline">\(\{1,1\}\)</span>.</p>
</div>
<div id="sampling-distribution-in-larger-samples" class="section level3 unnumbered hasAnchor">
<h3>Sampling distribution in larger samples<a href="statistics-refresher.html#sampling-distribution-in-larger-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now consider slightly larger samples. In Figure <a href="statistics-refresher.html#fig:diceroll10">1.2</a> the sample size is ten dice rolls.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diceroll10"></span>
<img src="docs/_bookdown_files/diceroll10.gif" alt="Sampling Distribution: 10 Dice Rolls per Sample" width="40%" />
<p class="caption">
Figure 1.2: Sampling Distribution: 10 Dice Rolls per Sample
</p>
</div>
<p>As was the case with samples of two dice rolls, we get a sampling distribution that is roughly symmetric around 3.5 and has a bell shape. What is different is that the distribution is a lot tighter around 3.5, which means that there are fewer extreme values than with samples of two dice. This should not be surprising. Consider a sample mean of 1, which only occurs if all dice yield one point. When we roll two dice, it is a lot more likely to have all ones than when we roll ten dice.</p>
<p>In Figure <a href="statistics-refresher.html#fig:diceroll100">1.3</a> we go even further and roll 100 dice at a time. Think about this like us pouring a bucket with 100 dice on the floor, calculating the mean, putting the dice back into the bucket, and starting again. Luckily, we can do this on a computer and don’t have to waste our time returning dice to a bucket.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diceroll100"></span>
<img src="docs/_bookdown_files/diceroll100.gif" alt="Sampling Distribution: 100 Dice Rolls per Sample" width="40%" />
<p class="caption">
Figure 1.3: Sampling Distribution: 100 Dice Rolls per Sample
</p>
</div>
<p>Again, the same picture emerges, namely a bell-shaped sampling distribution that is centered on the population mean of 3.5. With a sample size of 100, almost all samples have a mean close to 3.5.
And if we were to increase the sample size even further, the sampling distribution would be even tighter around the population mean of 3.5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:histsampling"></span>
<img src="_main_files/figure-html/histsampling-1.png" alt="Sampling Distribution of Means (2, 10, 100 dice rolls, 200 samples each)" width="672" />
<p class="caption">
Figure 1.4: Sampling Distribution of Means (2, 10, 100 dice rolls, 200 samples each)
</p>
</div>
<p>In Figure <a href="statistics-refresher.html#fig:histsampling">1.4</a>, we can see the histograms of the three sampling distributions. The center of all distributions is close to the expected value of <span class="math inline">\(3.5\)</span>. The distribution of the smallest sample size (2 dice rolls) is very wide whereas the distribution of samples with 100 dice rolls is fairly tight.</p>
<div class="blackbox">
<div class="center">
<p><strong>Key Observation</strong></p>
</div>
<p><strong>As the sample size <span class="math inline">\(n\)</span> increases</strong></p>
<ul>
<li>the <em>mean of each sample</em> gets <em>closer to the expected value</em> of <span class="math inline">\(3.5\)</span></li>
<li>the <em>sampling distribution</em> of the means becomes tighter around the expected value of <span class="math inline">\(3.5\)</span></li>
</ul>
<div class="center">
<p><strong>Explanation</strong></p>
</div>
<p>With a <em>small sample size <span class="math inline">\(n\)</span></em>, any particular sample may be very different from the population (where each side of the dice is equally likely to occur). We may have samples with only ones or only sixes.
As the <em>sample size <span class="math inline">\(n\)</span> increases</em> each individual sample looks more and more similar to the population and, thus, the sample mean becomes more similar to the population mean. This means that <strong>if we have a larger sample, we can be more confident</strong> that this sample can teach us something about the population.</p>
</div>
<p><span class="math display">\[\\[0.3cm]\]</span></p>
<p>The question is when is a sample <em>large enough</em>? The answer to this question unfortunately is <em>it depends</em>. We will later learn that samples greater than <span class="math inline">\(30\)</span> tend to have favourable properties, but in empirical applications we would need much greater samples than that.</p>
</div>
<div id="sampling-distribution-bottom-line" class="section level3 unnumbered hasAnchor">
<h3>Sampling Distribution: Bottom Line<a href="statistics-refresher.html#sampling-distribution-bottom-line" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we draw many random samples from a population and calculate a statistic in each random sample (such as the mean, variance, or the correlation between two variables), we will get <strong>a different value for this statistic in each sample</strong>. The distribution of these statistics is called the <strong>sampling distribution</strong>. If we have true random sampling, the sampling distribution of the mean has the following properties (for an infinite number of samples of size <span class="math inline">\(n\)</span>):</p>
<ul>
<li>Sampling mean: <span class="math inline">\(E(\overline{Y})=\mu_Y\)</span></li>
<li>Sampling variance <span class="math inline">\(Var(\overline{Y})=\frac{\sigma_Y^2}{n}\)</span>.</li>
</ul>
<p>These properties are very useful for statistical analysis because they tell us that the <strong>sampling distribution of the mean always follows the same logic</strong> regardless whether the underlying variable is points of a dice roll or a coin flip, a person’s height, IQ, or wage. As long as we draw random samples, we know what the mean and variance of the sampling distribution look like.</p>
</div>
</div>
<div id="the-law-of-large-numbers-and-consistency-of-an-estimator" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> The Law of Large Numbers and Consistency of an Estimator<a href="statistics-refresher.html#the-law-of-large-numbers-and-consistency-of-an-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What we observed in Figure <a href="statistics-refresher.html#fig:histsampling">1.4</a> is the result of the <strong>Law of Large Numbers (LLN)</strong>. According to the LLN, if we take an infinite number of random samples and take the sample mean <span class="math inline">\(\overline{Y}\)</span> in each sample, then the average of all sample means, <span class="math inline">\(E(\overline{Y})\)</span> converges to the true population mean <span class="math inline">\(\mu_Y\)</span>.</p>
<p>We can see this in Figure <a href="statistics-refresher.html#fig:histsampling">1.4</a>. Even with 200 small samples of size 2, the mean of the distribution is close to 3.5. If we were to draw more samples, it would get even closer to the population mean of 3.5. The LLN also implies that as the sample size <span class="math inline">\(n\)</span> increases, each sample mean has a higher probability of being close to the population mean <span class="math inline">\(\mu_Y\)</span>. We can see this in Figure <a href="statistics-refresher.html#fig:histsampling">1.4</a>: the larger <span class="math inline">\(n\)</span> is, the tigther is the distribution around the population mean of 3.5. If the sample was infinitely large, the distribution would collapse onto the vertical line at the population mean of 3.5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:llngraph"></span>
<img src="docs/_bookdown_files/lln.gif" alt="Illustration of the LLN" width="50%" />
<p class="caption">
Figure 1.5: Illustration of the LLN
</p>
</div>
<p>Figure <a href="statistics-refresher.html#fig:llngraph">1.5</a> illustrates the LLN based on dice rolls with sample sizes 2, 10, 50, 100 and 1000. Each dot is a sample mean, and if two samples have a similar mean we draw dots next to each other. For each sample size, we draw 200 samples. Again, we can make <strong>two observations that reflect the LLN</strong>:</p>
<ul>
<li>no matter what the sample size is, the <strong>mean of the distribution is close to the population mean (grey line)</strong>. You can see this because all distributions are roughly symmetric around 3.5.</li>
<li>As the <strong>sample size gets larger</strong>, the individual <strong>sample means are getting closer to the population mean</strong>, i.e. the distribution becomes tighter around the population mean.</li>
</ul>
</div>
<div id="the-central-limit-theorem" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> The Central Limit Theorem<a href="statistics-refresher.html#the-central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have established what the mean and variance of the sampling distribution is. But we can go even a step further and determine the shape of the distribution. To do that, we make use of the <strong>Central Limit Theorem (CLT)</strong>.</p>
<div class="blackbox">
<div class="center">
<p><strong>Definition: Central Limit Theorem</strong></p>
</div>
<p><strong>Wikipedia says</strong>: <em>The central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed..</em></p>
<p><strong>In plain English</strong>: Suppose we draw many random samples and in each sample we compute the mean. If the <strong>sample size is large enough</strong>, the <strong>sampling distribution is approximately normal</strong> (this holds for the mean and many other interesting statistics but there are some statistics where the CLT doesn’t hold.)</p>
</div>
<p>Formally, for the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> the CLT implies</p>
<p><span class="math display">\[\begin{equation*}
\overline{Y} \sim N(\mu_Y, sd=\frac{\sigma_Y}{\sqrt{n}}),
\end{equation*}\]</span></p>
<p>
that is, the sample means are normally distributed with mean <span class="math inline">\(\mu_Y\)</span> and standard deviation <span class="math inline">\(\sigma_Y/\sqrt{n}\)</span>.</p>
<div id="clt-animations" class="section level3 unnumbered hasAnchor">
<h3>CLT Animations<a href="statistics-refresher.html#clt-animations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To get a feel for the implications of the Central Limit Theorem, let’s look at some animations. Let’s start with a very small sample size (n=2) in Figure (fig:clt2). With every sample of two dice rolls, we add the mean to the distribution. The histogram shows the relative frequency of each value on the x-axis (i.e. a percentage). For example, if the first mean is 3, then the bin at 3 shows a relative frequency of 100%. If the second sample mean is 5, we add this to the histogram, and we now have a relative frequency of 50% at 3 and 50% at 5. We do this 200 times and see to what extent the histogram converges to the normal distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:clt2"></span>
<img src="docs/_bookdown_files/diceclt2.gif" alt="CLT: samples of 2 dice rolls" width="40%" />
<p class="caption">
Figure 1.6: CLT: samples of 2 dice rolls
</p>
</div>
<p>As we can see, the histogram does not converge to the normal distribution. It is roughly symmetric and bell-shaped but does not follow a normal distribution.</p>
<p>In Figure (fig:clt10), we consider sample sizes of 10 and 20, respectively. In both cases the histogram moves closer to a normal distribution. Statistical theory tells us that a sample size of 20 is still too small to assume normality. A rule of thumb is that a sample should be larger than 30 observations to justify applying a normal distribution.</p>
<p><img src="docs/_bookdown_files/diceclt10.gif" width="35%" style="display: block; margin: auto;" /><img src="docs/_bookdown_files/diceclt20.gif" width="35%" style="display: block; margin: auto;" /></p>
<p>In Figure (fig:clt50) we see an animation with samples of 50 dice rolls. Note that the range on the x-axis is narrower here. As we add more and more samples, the histogram gets very close to a normal distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:clt50"></span>
<img src="docs/_bookdown_files/diceclt50.gif" alt="CLT: samples of 50 dice rolls" width="40%" />
<p class="caption">
Figure 1.7: CLT: samples of 50 dice rolls
</p>
</div>
</div>
<div id="what-is-the-clt-useful-for" class="section level3 unnumbered hasAnchor">
<h3>What is the CLT Useful for?<a href="statistics-refresher.html#what-is-the-clt-useful-for" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to learn something about the population but typically we can only work with a sample. Collecting samples is often expensive, which means that we often only have one sample. If we want to know how much we can learn from this one sample about the population, we need to know what the sampling distribution looks like. This is entirely hypothetical: if we could draw many more samples from the same population, what would the sampling distribution look like? The Central Limit Theorem, along with the Law of Large Numbers can help us make sense of the sampling distribution. We know that its mean coincides with the population mean and we know that it is approximately normally distributed with variance <span class="math inline">\(\sigma_Y/\sqrt{n}\)</span>.</p>
<p>Most students do not fully embrace how important this insight is and why the CLT makes our life so much easier. No matter what the distribution of <span class="math inline">\(Y\)</span> is, that is, no matter what context or research question we study, thanks to the CLT we always know what the sampling distribution looks like. This means that we can use the same statistical analysis tools no matter what population we study.
What we typically do is “standardise” the sampling distribution such that the standardised distribution is normal with mean zero and standard deviation one. We do this by subtracting the population mean and dividing by the standard deviation,</p>
<p><span class="math display">\[\begin{equation*}
\frac{\overline{Y}-\mu_Y}{\sigma_Y/(\sqrt{n})} \sim N(0,1).
\end{equation*}\]</span></p>
<p>When we “standardise” <span class="math inline">\(\overline{Y}\)</span>, we ensure that all the sampling distributions have mean zero and a standard deviation of 1, regardless of the characteristics of the underlying random variable <span class="math inline">\(Y\)</span> and the sample size. The reason for doing this is a practical one. If we did not standardise <span class="math inline">\(\overline{Y}\)</span>, we would have to apply a different normal distribution – with a different mean and variance – in every analysis. By standardising, we can move this part to auto-pilot. We can use the same standard normal distribution no matter what we study.</p>
<p>Figure (fig:cltall) compares the sampling distribution of dice rolls with varying sample sizes to a normal distribution, N(0,1). Each picture shows the distribution of 1,000 samples of a given size, ranging from n=2 to n=200. Once we have sample sizes of 100 and larger, the histogram gets very close to a normal distribution and it would even get closer if we plotted more than 1,000 samples.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cltall"></span>
<img src="docs/_bookdown_files/clt.gif" alt="CLT: sampling distribution vs. N(0,1)" width="40%" />
<p class="caption">
Figure 1.8: CLT: sampling distribution vs. N(0,1)
</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-stats.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
